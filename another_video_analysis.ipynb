{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "another video analysis.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fvndnv/vdieo-analysis/blob/master/another_video_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sXIWBzB8dByQ",
        "colab_type": "code",
        "outputId": "f625f4fd-f886-4618-a014-dae534adfea9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ylsNsbicdJTC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "df=pd.read_csv('/content/drive/My Drive/ucfTrainTestlist/trainlist01.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T6YBpBtZfHaZ",
        "colab_type": "code",
        "outputId": "7cda58dc-bdb6-40a7-ed34-46fd410d695a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ApplyEyeMakeup/v_ApplyEyeMakeup_g08_c01.avi 1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ApplyEyeMakeup/v_ApplyEyeMakeup_g08_c02.avi 1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ApplyEyeMakeup/v_ApplyEyeMakeup_g08_c03.avi 1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ApplyEyeMakeup/v_ApplyEyeMakeup_g08_c04.avi 1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ApplyEyeMakeup/v_ApplyEyeMakeup_g08_c05.avi 1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ApplyEyeMakeup/v_ApplyEyeMakeup_g09_c01.avi 1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   ApplyEyeMakeup/v_ApplyEyeMakeup_g08_c01.avi 1\n",
              "0  ApplyEyeMakeup/v_ApplyEyeMakeup_g08_c02.avi 1\n",
              "1  ApplyEyeMakeup/v_ApplyEyeMakeup_g08_c03.avi 1\n",
              "2  ApplyEyeMakeup/v_ApplyEyeMakeup_g08_c04.avi 1\n",
              "3  ApplyEyeMakeup/v_ApplyEyeMakeup_g08_c05.avi 1\n",
              "4  ApplyEyeMakeup/v_ApplyEyeMakeup_g09_c01.avi 1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eSki8Coom66W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GjMBVHdnnM5a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "from keras.layers import Dense,Dropout,InputLayer\n",
        "from skimage.transform import resize\n",
        "from keras.preprocessing import image\n",
        "from sklearn.model_selection import train_test_split\n",
        "from glob import glob\n",
        "from tqdm import tqdm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1NjgmHSn0LB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2qEWnDESn9-c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.utils import np_utils"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uBSTGMA-oDuQ",
        "colab_type": "code",
        "outputId": "49b6090b-83c4-4845-f5f2-b30bb5d5b39d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "f=open(\"/content/drive/My Drive/ucfTrainTestlist/trainlist01.txt\", \"r\")\n",
        "temp=f.read()\n",
        "videos=temp.split('\\n')\n",
        "train=pd.DataFrame()\n",
        "train['videogames']=videos\n",
        "train=train[:-1]\n",
        "train.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>videogames</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ApplyEyeMakeup/v_ApplyEyeMakeup_g08_c01.avi 1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ApplyEyeMakeup/v_ApplyEyeMakeup_g08_c02.avi 1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ApplyEyeMakeup/v_ApplyEyeMakeup_g08_c03.avi 1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ApplyEyeMakeup/v_ApplyEyeMakeup_g08_c04.avi 1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ApplyEyeMakeup/v_ApplyEyeMakeup_g08_c05.avi 1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                      videogames\n",
              "0  ApplyEyeMakeup/v_ApplyEyeMakeup_g08_c01.avi 1\n",
              "1  ApplyEyeMakeup/v_ApplyEyeMakeup_g08_c02.avi 1\n",
              "2  ApplyEyeMakeup/v_ApplyEyeMakeup_g08_c03.avi 1\n",
              "3  ApplyEyeMakeup/v_ApplyEyeMakeup_g08_c04.avi 1\n",
              "4  ApplyEyeMakeup/v_ApplyEyeMakeup_g08_c05.avi 1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w66pvPHfoMkb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## This is how the names of videos are given in the .txt file. It is not properly aligned and we will need to preprocess it. \n",
        "## Before that, let’s create a similar dataframe for test videos as well:"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F6GYFhcppLh-",
        "colab_type": "code",
        "outputId": "0eca7501-f633-4b23-b08d-64253bfc70bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "f=open(\"/content/drive/My Drive/ucfTrainTestlist/testlist01.txt\", \"r\")\n",
        "temp=f.read()\n",
        "videos=temp.split('\\n')\n",
        "test=pd.DataFrame()\n",
        "test['videogames']=videos\n",
        "test=test[:-1]\n",
        "test.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>videogames</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ApplyEyeMakeup/v_ApplyEyeMakeup_g01_c01.avi</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ApplyEyeMakeup/v_ApplyEyeMakeup_g01_c02.avi</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ApplyEyeMakeup/v_ApplyEyeMakeup_g01_c03.avi</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ApplyEyeMakeup/v_ApplyEyeMakeup_g01_c04.avi</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ApplyEyeMakeup/v_ApplyEyeMakeup_g01_c05.avi</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                    videogames\n",
              "0  ApplyEyeMakeup/v_ApplyEyeMakeup_g01_c01.avi\n",
              "1  ApplyEyeMakeup/v_ApplyEyeMakeup_g01_c02.avi\n",
              "2  ApplyEyeMakeup/v_ApplyEyeMakeup_g01_c03.avi\n",
              "3  ApplyEyeMakeup/v_ApplyEyeMakeup_g01_c04.avi\n",
              "4  ApplyEyeMakeup/v_ApplyEyeMakeup_g01_c05.avi"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QW1FeQmppdWw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Next, we will add the tag of each video (for both training and test sets).\n",
        "### Did you notice that the entire part before the ‘/’ in the video name represents the tag of the video? Hence, we will split the entire string on ‘/’ and select the tag for all the videos:"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ojYnuIJSp_Jy",
        "colab_type": "code",
        "outputId": "83cb111b-f04d-4d66-d61d-a97328c55c5f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train.shape,  train.shape[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((9537, 1), 9537)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kRosVLZ2ptP4",
        "colab_type": "code",
        "outputId": "f93315b4-caaf-49dc-b801-091359f6305a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# creating tags for training videos\n",
        "tttrain_video_tag=[]\n",
        "for i in range(train.shape[0]):\n",
        "  tttrain_video_tag.append(train['videogames'][i].split('/'))\n",
        "\n",
        "tttrain_video_tag[:2]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['ApplyEyeMakeup', 'v_ApplyEyeMakeup_g08_c01.avi 1'],\n",
              " ['ApplyEyeMakeup', 'v_ApplyEyeMakeup_g08_c02.avi 1']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y-7pH6hzqspN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# creating tags for training videos\n",
        "train_video_flag=[]\n",
        "for i in range(train.shape[0]):\n",
        "  train_video_flag.append(train['videogames'][i].split('/')[0])\n",
        "train['tag']=train_video_flag\n",
        "\n",
        "### creating tags for test videos\n",
        "\n",
        "test_video_flag=[]\n",
        "for i in range(test.shape[0]):\n",
        "  test_video_flag.append(test['videogames'][i].split('/')[0])\n",
        "test['tag']=test_video_flag\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TBO-yPP9rfsp",
        "colab_type": "code",
        "outputId": "2c139db0-306c-4a16-cdc2-41247b3a7559",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train.shape,    test.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((9537, 2), (3783, 2))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ij3r4tTqsSL7",
        "colab_type": "code",
        "outputId": "926883c8-4616-4d7b-c7ea-c839b3350aed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "train.columns, test.columns"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(Index(['videogames', 'tag'], dtype='object'),\n",
              " Index(['videogames', 'tag'], dtype='object'))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nYPQYPkesU5e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# So what’s next? Now, we will extract the frames from the training videos which will be used to train the model. I will be storing all the frames in a folder named train_1.\n",
        "\n",
        "# So, first of all, make a new folder and rename it to ‘train_1’ and then follow the code given below to extract frames:"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aUl_umiGsgxK",
        "colab_type": "code",
        "outputId": "b95ee6f2-5712-4dbc-ac8c-911531f4b09e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "# storing the frames from training videos\n",
        "for i in tqdm(range(train.shape[0])):\n",
        "    count = 0\n",
        "    videoFile = train['videogames'][i]\n",
        "    cap = cv2.VideoCapture('UCF/'+videoFile.split(' ')[0].split('/')[1])   # capturing the video from the given path\n",
        "    frameRate = cap.get(5) #frame rate\n",
        "    x=1\n",
        "    while(cap.isOpened()):\n",
        "        frameId = cap.get(1) #current frame number\n",
        "        ret, frame = cap.read()\n",
        "        if (ret != True):\n",
        "            break\n",
        "        if (frameId % math.floor(frameRate) == 0):\n",
        "            # storing the frames in a new folder named train_1\n",
        "            filename ='train_1/' + videoFile.split('/')[1].split(' ')[0] +\"_frame%d.jpg\" % count;count+=1\n",
        "            cv2.imwrite(filename, frame)\n",
        "    cap.release()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/9537 [00:00<?, ?it/s]\u001b[A\n",
            " 20%|██        | 1910/9537 [00:00<00:00, 19098.65it/s]\u001b[A\n",
            " 41%|████      | 3926/9537 [00:00<00:00, 19403.50it/s]\u001b[A\n",
            " 62%|██████▏   | 5876/9537 [00:00<00:00, 19432.23it/s]\u001b[A\n",
            " 82%|████████▏ | 7794/9537 [00:00<00:00, 19354.84it/s]\u001b[A\n",
            "100%|██████████| 9537/9537 [00:00<00:00, 19470.68it/s]\u001b[A"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SFR29exsvWjB",
        "colab_type": "text"
      },
      "source": [
        "This will take some time as there are more than 9,500 videos in the training set. Once the frames are extracted, we will save the name of these frames with their corresponding tag in a .csv file. Creating this file will help us to read the frames which we will see in the next section."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jyV4WagtvXeI",
        "colab_type": "code",
        "outputId": "b5dbcd6f-173d-4bc4-8a0e-b8a1519d0caa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "images = glob(\"train_1/*.jpg\")\n",
        "train_image = []\n",
        "train_class = []\n",
        "for i in tqdm(range(len(images))):\n",
        "    # creating the image name\n",
        "    train_image.append(images[i].split('/')[1])\n",
        "    # creating the class of image\n",
        "    train_class.append(images[i].split('/')[1].split('_')[1])\n",
        "    \n",
        "# storing the images and their class in a dataframe\n",
        "train_data = pd.DataFrame()\n",
        "train_data['image'] = train_image\n",
        "train_data['class'] = train_class\n",
        "\n",
        "# converting the dataframe into csv file \n",
        "train_data.to_csv('/content/drive/My Drive/ucfTrainTestlist/train_new.csv',header=True, index=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "0it [00:00, ?it/s]\u001b[A\n",
            "\u001b[A"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CKAKSFlyv3SK",
        "colab_type": "code",
        "outputId": "4b75e32f-d610-40f1-fd41-fdfbf6533ee0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49
        }
      },
      "source": [
        "t=pd.read_csv(\"/content/drive/My Drive/ucfTrainTestlist/train_new.csv\")\n",
        "t.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image</th>\n",
              "      <th>class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [image, class]\n",
              "Index: []"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OIwAAGxuRqr_",
        "colab_type": "text"
      },
      "source": [
        "So far, we have extracted frames from all the training videos and saved them in a .csv file along with their corresponding tags. It’s now time to train our model which we will use to predict the tags for videos in the test set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-bLGaXTR4Nr",
        "colab_type": "text"
      },
      "source": [
        "Training the Video Classification Model\n",
        "It’s finally time to train our video classification model! I’m sure this is the most anticipated section of the tutorial. I have divided this step into sub-steps for ease of understanding:\n",
        "\n",
        "Read all the frames that we extracted earlier for the training images\n",
        "Create a validation set which will help us examine how well our model will perform on unseen data\n",
        "Define the architecture of our model\n",
        "Finally, train the model and save its weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWspSqD2R_-p",
        "colab_type": "text"
      },
      "source": [
        "Reading all the video frames\n",
        "So, let’s get started with the first step where we will extract the frames. We will import the libraries first:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZl3ojFNSAj_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "outputId": "663afb66-a12e-42dc-a7cf-bae79ee076b8"
      },
      "source": [
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.layers import Dense, InputLayer, Dropout, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D, GlobalMaxPooling2D\n",
        "from keras.preprocessing import image\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2bNd_jIGSD9u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = pd.read_csv('UCF/train_new.csv')\n",
        "train.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dU6tOvQXSWnv",
        "colab_type": "text"
      },
      "source": [
        "This is how the first five rows look like. We have the corresponding class or tag for each frame. Now, using this .csv file, we will read the frames that we extracted earlier and then store those frames as a NumPy array:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pRYDrIamSXaV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# creating an empty list\n",
        "train_image = []\n",
        "\n",
        "# for loop to read and store frames\n",
        "for i in tqdm(range(train.shape[0])):\n",
        "    # loading the image and keeping the target size as (224,224,3)\n",
        "    img = image.load_img('train_1/'+train['image'][i], target_size=(224,224,3))\n",
        "    # converting it to array\n",
        "    img = image.img_to_array(img)\n",
        "    # normalizing the pixel value\n",
        "    img = img/255\n",
        "    # appending the image to the train_image list\n",
        "    train_image.append(img)\n",
        "    \n",
        "# converting the list to numpy array\n",
        "X = np.array(train_image)\n",
        "\n",
        "# shape of the array\n",
        "X.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FT_zzzKKS086",
        "colab_type": "text"
      },
      "source": [
        "Output: (73844, 224, 224, 3)\n",
        "\n",
        "We have 73,844 images each of size (224, 224, 3). Next, we will create the validation set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AYa2FoyaS9-7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# separating the target\n",
        "y = train['class']\n",
        "\n",
        "# creating the training and validation set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.2, stratify = y)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9UsTEFacVb2i",
        "colab_type": "text"
      },
      "source": [
        "Here, stratify = y (which is the class or tags of each frame) keeps the similar distribution of classes in both the training as well as the validation set.\n",
        "\n",
        "Remember – there are 101 categories in which a video can be classified. So, we will have to create 101 different columns in the target, one for each category. We will use the get_dummies() function for that:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t5g2jD1BVcoE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# creating dummies of target variable for train and validation set\n",
        "y_train = pd.get_dummies(y_train)\n",
        "y_test = pd.get_dummies(y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SfHFkfIkVngb",
        "colab_type": "text"
      },
      "source": [
        "Defining the architecture of the video classification model\n",
        "Since we do not have a very large dataset, creating a model from scratch might not work well. So, we will use a pre-trained model and take its learnings to solve our problem.\n",
        "\n",
        "For this particular dataset, we will be using the VGG-16 pre-trained model. Let’s create a base model of the pre-trained model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9XJGknwTVn92",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# creating the base model of pre-trained VGG16 model\n",
        "base_model = VGG16(weights='imagenet', include_top=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1_wm-zpVxoR",
        "colab_type": "text"
      },
      "source": [
        "This model was trained on a dataset that has 1,000 classes. We will fine tune this model as per our requirement. include_top = False will remove the last layer of this model so that we can tune it as per our need.\n",
        "\n",
        "Now, we will extract features from this pre-trained model for our training and validation images:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8taoVogVyYy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# extracting features for training frames\n",
        "X_train = base_model.predict(X_train)\n",
        "X_train.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v6hmiIg8V3aS",
        "colab_type": "text"
      },
      "source": [
        "Output: (59075, 7, 7, 512)\n",
        "\n",
        "We have 59,075 images in the training set and the shape has been changed to (7, 7, 512) since we have passed these images through the VGG16 architecture. Similarly, we will extract features for validation frames:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HKJ7WI-TV8d0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# extracting features for validation frames\n",
        "X_test = base_model.predict(X_test)\n",
        "X_test.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rnzKbs1RWIL4",
        "colab_type": "text"
      },
      "source": [
        "Output: (14769, 7, 7, 512)\n",
        "\n",
        "There are 14,769 images in the validation set and the shape of these images has also changed to (7, 7, 512). We will use a fully connected network now to fine-tune the model. This fully connected network takes input in single dimension. So, we will reshape the images into a single dimension:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "it1X1XPtWIn0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# reshaping the training as well as validation frames in single dimension\n",
        "X_train = X_train.reshape(59075, 7*7*512)\n",
        "X_test = X_test.reshape(14769, 7*7*512)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9AGmAu9TWPwr",
        "colab_type": "text"
      },
      "source": [
        "It is always advisable to normalize the pixel values, i.e., keep the pixel values between 0 and 1. This helps the model to converge faster."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CJpCWAfcWQqO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# normalizing the pixel values\n",
        "max = X_train.max()\n",
        "X_train = X_train/max\n",
        "X_test = X_test/max"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "psZZR4sAWhWU",
        "colab_type": "text"
      },
      "source": [
        "Next, we will create the architecture of the model. We have to define the input shape for that. So, let’s check the shape of our images:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fWVBYYm6Wh1S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# shape of images\n",
        "X_train.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-SfUIgOWpFC",
        "colab_type": "text"
      },
      "source": [
        "Output: (59075, 25088)\n",
        "\n",
        "The input shape will be 25,088. Let’s now create the architecture:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3a22Ehc-Wp0J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#defining the model architecture\n",
        "model = Sequential()\n",
        "model.add(Dense(1024, activation='relu', input_shape=(25088,)))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(101, activation='softmax'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VR-XUrE9W58t",
        "colab_type": "text"
      },
      "source": [
        "Training the video classification model\n",
        "We will now train our model using the training frames and validate the model using validation frames. We will save the weights of the model so that we will not have to retrain the model again and again.\n",
        "\n",
        "So, let’s define a function to save the weights of the model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xfWta20VW6Y7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# defining a function to save the weights of best model\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "mcp_save = ModelCheckpoint('weight.hdf5', save_best_only=True, monitor='val_loss', mode='min')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HAeha3cpXBge",
        "colab_type": "text"
      },
      "source": [
        "We will decide the optimum model based on the validation loss. Note that the weights will be saved as weights.hdf5. You can rename the file if you wish. Before training the model, we have to compile it:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8badUkAOXB9W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# compiling the model\n",
        "model.compile(loss='categorical_crossentropy',optimizer='Adam',metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q8TO4vx7XJtk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# training the model\n",
        "model.fit(X_train, y_train, epochs=200, validation_data=(X_test, y_test), callbacks=[mcp_save], batch_size=128)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bNZE0RU5yUYZ",
        "colab_type": "text"
      },
      "source": [
        "I have trained the model for 200 epochs. To download the weights which I got after training the model, you can use this link.\n",
        "\n",
        "We now have the weights which we will use to make predictions for the new videos. So, in the next section, we will see how well this model will perform on the task of video classification!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Kt6XBiyybMl",
        "colab_type": "text"
      },
      "source": [
        "Evaluating our Video Classification Model\n",
        "Let’s open a new Jupyter notebook to evaluate the model. The evaluation part can also be split into multiple steps to understand the process more clearly:\n",
        "\n",
        "Define the model architecture and load the weights\n",
        "Create the test data\n",
        "Make predictions for the test videos\n",
        "Finally, evaluate the model\n",
        " \n",
        "\n",
        "Defining model architecture and loading weights\n",
        "You’ll be familiar with the first step – importing the required libraries:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R8FJaFVXybmZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.preprocessing import image\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from keras.applications.vgg16 import VGG16\n",
        "import cv2\n",
        "import math\n",
        "import os\n",
        "from glob import glob\n",
        "from scipy import stats as s"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bhPXhf9dyh6z",
        "colab_type": "text"
      },
      "source": [
        "Next, we will define the model architecture which will be similar to what we had while training the model:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y1V3GLiqyiL5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "base_model = VGG16(weights='imagenet', include_top=False)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bz4fPqvfyn-X",
        "colab_type": "text"
      },
      "source": [
        "This is the pre-trained model and we will fine-tune it next:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E14qFz7tyooE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#defining the model architecture\n",
        "model = Sequential()\n",
        "model.add(Dense(1024, activation='relu', input_shape=(25088,)))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(101, activation='softmax'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cLI8TwdyukE",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Now, as we have defined the architecture, we will now load the trained weights which we stored as weights.hdf5:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kzcvvWhRyw9F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.load_weights(\"weights.hdf5\")\n",
        "model.compile(loss='categorical_crossentropy',optimizer='Adam',metrics=['accuracy'])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XsFa_-xUy7vU",
        "colab_type": "text"
      },
      "source": [
        "Make sure that the loss function, optimizer, and the metrics are the same as we used while training the model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "02IMifLOy8FM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Creating the test data\n",
        "You should have downloaded the train/test split files as per the official documentation of the UCF101 dataset. If not, download it from here. In the downloaded folder, there is a file named “testlist01.txt” which contains the list of test videos. We will make use of that to create the test data:"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YwazhC15zD8p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# getting the test list\n",
        "f = open(\"testlist01.txt\", \"r\")\n",
        "temp = f.read()\n",
        "videos = temp.split('\\n')\n",
        "\n",
        "# creating the dataframe\n",
        "test = pd.DataFrame()\n",
        "test['video_name'] = videos\n",
        "test = test[:-1]\n",
        "test_videos = test['video_name']\n",
        "test.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EAwyCjjtzSTx",
        "colab_type": "text"
      },
      "source": [
        "We now have the list of all the videos stored in a dataframe. To map the predicted categories with the actual categories, we will use the train_new.csv file:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lv7eJWumzSrQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# creating the tags\n",
        "train = pd.read_csv('UCF/train_new.csv')\n",
        "y = train['class']\n",
        "y = pd.get_dummies(y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H6p9BosmzXK8",
        "colab_type": "text"
      },
      "source": [
        "Now, we will make predictions for the videos in the test set.\n",
        "\n",
        "Generating predictions for test videos\n",
        "Let me summarize what we will be doing in this step before looking at the code. The below steps will help you understand the prediction part:\n",
        "\n",
        "First, we will create two empty lists – one to store the predictions and the other to store the actual tags\n",
        "Then, we will take each video from the test set, extract frames for this video and store it in a folder (create a folder named temp in the current directory to store the frames). We will remove all the other files from this folder at each iteration\n",
        "Next, we will read all the frames from the temp folder, extract features for these frames using the pre-trained model, predict tags, and then take the mode to assign a tag for that particular video and append it in the list\n",
        "We will append actual tags for each video in the second list\n",
        "Let’s code these steps and generate predictions:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9PSTuEVrzof6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# creating two lists to store predicted and actual tags\n",
        "predict = []\n",
        "actual = []\n",
        "\n",
        "# for loop to extract frames from each test video\n",
        "for i in tqdm(range(test_videos.shape[0])):\n",
        "    count = 0\n",
        "    videoFile = test_videos[i]\n",
        "    cap = cv2.VideoCapture('UCF/'+videoFile.split(' ')[0].split('/')[1])   # capturing the video from the given path\n",
        "    frameRate = cap.get(5) #frame rate\n",
        "    x=1\n",
        "    # removing all other files from the temp folder\n",
        "    files = glob('temp/*')\n",
        "    for f in files:\n",
        "        os.remove(f)\n",
        "    while(cap.isOpened()):\n",
        "        frameId = cap.get(1) #current frame number\n",
        "        ret, frame = cap.read()\n",
        "        if (ret != True):\n",
        "            break\n",
        "        if (frameId % math.floor(frameRate) == 0):\n",
        "            # storing the frames of this particular video in temp folder\n",
        "            filename ='temp/' + \"_frame%d.jpg\" % count;count+=1\n",
        "            cv2.imwrite(filename, frame)\n",
        "    cap.release()\n",
        "    \n",
        "    # reading all the frames from temp folder\n",
        "    images = glob(\"temp/*.jpg\")\n",
        "    \n",
        "    prediction_images = []\n",
        "    for i in range(len(images)):\n",
        "        img = image.load_img(images[i], target_size=(224,224,3))\n",
        "        img = image.img_to_array(img)\n",
        "        img = img/255\n",
        "        prediction_images.append(img)\n",
        "        \n",
        "    # converting all the frames for a test video into numpy array\n",
        "    prediction_images = np.array(prediction_images)\n",
        "    # extracting features using pre-trained model\n",
        "    prediction_images = base_model.predict(prediction_images)\n",
        "    # converting features in one dimensional array\n",
        "    prediction_images = prediction_images.reshape(prediction_images.shape[0], 7*7*512)\n",
        "    # predicting tags for each array\n",
        "    prediction = model.predict_classes(prediction_images)\n",
        "    # appending the mode of predictions in predict list to assign the tag to the video\n",
        "    predict.append(y.columns.values[s.mode(prediction)[0][0]])\n",
        "    # appending the actual tag of the video\n",
        "    actual.append(videoFile.split('/')[1].split('_')[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1z1wvjOp4IRj",
        "colab_type": "text"
      },
      "source": [
        "This step will take some time as there are around 3,800 videos in the test set. Once we have the predictions, we will calculate the performance of the model.\n",
        "\n",
        " \n",
        "\n",
        "Evaluating the model\n",
        "Time to evaluate our model and see what all the fuss was about.\n",
        "\n",
        "We have the actual tags as well as the tags predicted by our model. We will make use of these to get the accuracy score. On the official documentation page of UCF101, the current accuracy is 43.90%. Can our model beat that? Let’s check!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZrDVLih4JIz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# checking the accuracy of the predicted tags\n",
        "from sklearn.metrics import accuracy_score\n",
        "accuracy_score(predict, actual)*100"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}